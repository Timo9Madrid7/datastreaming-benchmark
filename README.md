# Messaging Benchmark Framework

This project provides an extensible framework to benchmark messaging technologies in controlled, repeatable conditions. The system is designed for internal experiments to evaluate latency, throughput, and overhead under various network and scenario configurations.

---

## ğŸ§­ Project Purpose

This tool exists to support **internal, comparative evaluation** of message-passing technologies. It enables structured experimentation across combinations of:
- Message sizes and frequencies
- Numbers of producers, consumers, and topics
- Subscription patterns and topic multiplexing
- Network conditions (latency, jitter, bandwidth, loss)

The output is a basis for comparing **performance**, **overhead**, and **behavioral characteristics** across technologies.

---

## ğŸ¯ High-Level Goals

1. **Isolate and measure technology overhead**  
   Capture the cost of using Kafka, ZeroMQ, etc., beyond the base network transport.

2. **Support modular experimentation**  
   New scenarios and technologies can be tested without modifying core logic.

3. **Automate end-to-end test orchestration**  
   Given a configuration file, the system launches the required containers, injects environment, collects logs, and tears down after execution.

---

## ğŸ§± Design Overview

### Architecture Principles

- **Separation of Concerns**
  - Core logic defines interfaces and orchestration, not implementation specifics.
  - Tech-specific code lives in loadable modules (`.so`, `.dll`).

- **Dynamic Factory + Shared Libs**
  - Messaging implementations register themselves at runtime via factories.

- **Technology Isolation via Docker**
  - One image per publisher/consumer pair per tech.
  - Shared `Dockerfile.base` for build dependencies.

- **Interface-Based Extensibility**
  - `IPublisher` and `IConsumer` define the contract.
  - Payloads are lightweight (`label + vector<double>`).

- **Scenario-as-Data**
  - Experiment dimensions (e.g. producers, size, rate, etc.) come from JSON.
  - The orchestrator uses these configs to coordinate container deployment.

---

## ğŸ§ª Experiment Flow

1. Define a test scenario in JSON (see `test_scenarios/quick_test.json` for template).
2. The orchestrator reads the config, generates combinations, and launches the matching containers.
3. All containers are paused at startup, then synchronized and unpaused together.
4. Metrics and events are logged.
5. Containers terminate on poison-pill signals and are then cleaned up.

---

## ğŸ—‚ Project Structure (Core-Only)

    core/                        # Contains common logic files and modules
    â”œâ”€â”€ analyses/                   # Contains notebooks to inspect retrieved experimental data
    â”‚   â”œâ”€â”€ messaging_stats.ipynb      # Visualize graphs based on messaging-related data from the experiments
    â”‚   â”œâ”€â”€ resources_stats.ipynb      # Visualize graphs based on resources usage data from the experiments
    â”œâ”€â”€ applications/               # PublisherApp and ConsumerApp (main executables)
    â”œâ”€â”€ factory/                    # Factory pattern logic for dynamic tech binding
    â”œâ”€â”€ interfaces/                 # Core abstractions: IPublisher, IConsumer
    â”œâ”€â”€ logger/                     # Logger implementation with level-based control
    â”œâ”€â”€ orchestrator/               # Python modules for scenario execution and orchestration
    â”‚   â”œâ”€â”€ benchmark_manager.py       # Main entry point for experiment lifecycle
    â”‚   â”œâ”€â”€ container_manager.py       # Handles Docker container management logic
    â”‚   â”œâ”€â”€ events_logger.py           # Retrieves messages logged by the containers
    â”‚   â”œâ”€â”€ metrics_collector.py       # Monitors system performance 
    â”‚   â”œâ”€â”€ scenario_manager.py        # Instantiates scenarios from JSON
    â”‚   â”œâ”€â”€ scenario_config_manager.py # Iterates and validates scenarios
    â”‚   â”œâ”€â”€ technology_manager.py      # Interface that technologies must implement beyond IPublisher and IConsumer 
    â”‚   â””â”€â”€technologies/               # Python module with technology-specific implementations of the TechnologyManager interface
    â”œâ”€â”€ payload/                    # Message structures that the benchmark supports
    â”œâ”€â”€ technology_loader/          # Handles technology-specific plugin dynamic loading
    â”œâ”€â”€ Dockerfile.base             # Base image with C++ build dependencies
    â”œâ”€â”€ Dockerfile.publisher        # Publisher-specific image (extends base)
    â”œâ”€â”€ Dockerfile.consumer         # Consumer-specific image (extends base)
    diagrams/                    # Contains diagrams defined using PlantUML to help understand the architecture
    logs/                        # Contains benchmark log files under their scenario_config json file and technology name folders.
    â”œâ”€â”€ quick_test/                 # Contains logs related to the benchmarking of quick_test scenario conditions
    â”‚   â”œâ”€â”€ kafka/                     # Contains logs from the Kafka implementation in this benchmarking experiment
    â”‚   â”œâ”€â”€ zeromq_p2p/                # Contains logs from the ZeroMQ implementation in this benchmarking experiment
    technologies/                # Contains technology-specific Docker images definition as well as implementations of IPublisher, IConsumer, and their registration logic
    â”œâ”€â”€ kafka/                      # Kafka implementation
    â”œâ”€â”€ zeromq_p2p/                 # ZeroMQ implementation
    â””â”€â”€ ...                         # Additional tech modules

    test_scenarios/              # Contains scenario configuration json files
    â”œâ”€â”€ quick_test.json             # Scenario config: topics, producers, rate, etc.
    â””â”€â”€ ...                         # Optional experimental configurations
    benchmark_scenarios.json     # Parameterizes scenario configuration and technologies to use in the benchmarking experiment
    build_all_images.bat         # Creates Docker images for all technologies. Extend as new technologies are  implemented
    execute_experiments.py       # Entry point to execute an experiment. Admits 2 parameters: mode and duration_messages. The first is a logger level as defined in core/logger. The second is a string used to filter subsets of the configurable scenarios based on their completion criteria.
    requirements.txt
    README.md
    setup_instructions.md

---

## âš™ï¸ Technologies

Each messaging technology lives in its own subdirectory under `technologies/`. Each implementation must:
- Extend `IPublisher` and `IConsumer`
- Register itself via the factory
- Compile into a shared object

Each also gets its own Dockerfiles for consumer/publisher images.

---

## ğŸ“Œ Notes

- Have the combinatory explosion threat in mind when designing scenario configuration dimension parameters.
- Technology-specific implementations belong to 2 scopes: their `technology` subfolder for messaging interfaces definition and registration, and in the `core/orchestrator/technology_loader/` folder for technology-specific global setup handling.
- The orchestrator uses Docker Python API â€” make sure there is access to the Docker daemon.

---

## ğŸ§  For Extension

When adding a new tech or extending the scenario model:
- Follow the interface and registration pattern â€” nothing in `core/` should need to change except for the addition of a new python module in `core/orchestrator/technology_loader/`.
- Keep Docker images lean: start from `Dockerfile.base`, add only whatâ€™s necessary.
- Update the orchestrator only if scenario structure or orchestration behavior needs to evolve.

---

