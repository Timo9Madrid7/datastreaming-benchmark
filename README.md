# Messaging Benchmark Framework

This project provides an extensible framework to benchmark messaging technologies in controlled, repeatable conditions. It is designed for internal experiments to evaluate latency, throughput, and overhead under varied network and scenario configurations while keeping experiments reproducible and comparable.

---

## ğŸ§­ Project Purpose

This tool exists to support **internal, comparative evaluation** of message-passing technologies. It enables structured experimentation across combinations of:
- Message sizes and frequencies
- Numbers of producers, consumers, and topics
- Subscription patterns and topic multiplexing
- Network conditions (latency, jitter, bandwidth, loss)

The output is a basis for comparing **performance**, **overhead**, and **behavioral characteristics** across technologies with consistent orchestration and logging.

---

## ğŸ¯ High-Level Goals

1. **Isolate and measure technology overhead**  
  Capture the cost of using Kafka, ZeroMQ, Arrow Flight, NATS, and RabbitMQ beyond the base network transport.

2. **Support modular experimentation**  
   New scenarios and technologies can be tested without modifying core logic.

3. **Automate end-to-end test orchestration**  
   Given a configuration file, the system launches the required containers, injects environment, collects logs, and tears down after execution.

---

## ğŸ§± Design Overview

### Architecture Principles

- **Separation of Concerns**
  - Core logic defines interfaces and orchestration, not implementation specifics.
  - Tech-specific code lives in loadable modules (`.so`, `.dll`).

- **Dynamic Factory + Shared Libs**
  - Messaging implementations register themselves at runtime via factories.

- **Technology Isolation via Docker**
  - One image per publisher/consumer pair per tech.
  - Shared `Dockerfile.base` for build dependencies and consistent build inputs.

- **Interface-Based Extensibility**
  - `IPublisher` and `IConsumer` define the contract.
  - Payloads are lightweight (`label + vector<double>`).

- **Scenario-as-Data**
  - Experiment dimensions (e.g., producers, size, rate) come from JSON.
  - The orchestrator uses these configs to coordinate container deployment.

---

## ğŸ§ª Experiment Flow

1. Define a test scenario in JSON (see `test_scenarios/quick_test.json` for a template).
2. The orchestrator reads the config, generates combinations, and launches the matching containers.
3. All containers are paused at startup, then synchronized and unpaused together.
4. Metrics and events are logged per scenario and technology.
5. Containers terminate on poison-pill signals and are then cleaned up.

---

## âœ… Quickstart

1. Follow the environment setup in `setup_instructions.md`.
2. Build all technology images with `build_all_images.bat`.
3. Run a scenario via `execute_experiments.py`.
4. Inspect results under `logs/` and use the `analysis/` helpers to plot metrics.

---

## ğŸ§° Prerequisites

- Docker (daemon accessible from the host running the orchestrator)
- CMake + a C++ toolchain for the core applications
- Python environment for the orchestrator and analysis helpers

---

## ğŸ§¾ Configuration

- `benchmark_scenarios.json` defines which technologies and scenario files to execute.
- `test_scenarios/` holds the scenario templates (message sizes, rates, topics, producers, consumers).
- Technology-specific container behavior is defined under `technologies/` and hooked through the orchestratorâ€™s technology loader.

---

## ğŸ—‚ Project Structure

    analysis/                     # Python utilities for loading results and building plots
    â”œâ”€â”€ data_loader.py              # Load experiment logs and scenario metadata
    â”œâ”€â”€ metrics.py                  # Compute derived metrics from raw logs
    â””â”€â”€ visuals.py                  # Charts and report visualizations
    core/                         # C++ core logic and orchestration tooling
    â”œâ”€â”€ applications/               # PublisherApp and ConsumerApp (main executables)
    â”œâ”€â”€ factory/                    # Factory pattern logic for dynamic tech binding
    â”œâ”€â”€ interfaces/                 # Core abstractions: IPublisher, IConsumer
    â”œâ”€â”€ logger/                     # Logger implementation with level-based control
    â”œâ”€â”€ orchestrator/               # Python modules for scenario execution and orchestration
    â”‚   â”œâ”€â”€ benchmark_manager.py       # Main entry point for experiment lifecycle
    â”‚   â”œâ”€â”€ container_manager.py       # Docker container management logic
    â”‚   â”œâ”€â”€ events_logger.py           # Retrieves messages logged by the containers
    â”‚   â”œâ”€â”€ metrics_collector.py       # Monitors system performance
    â”‚   â”œâ”€â”€ scenario_manager.py        # Instantiates scenarios from JSON
    â”‚   â”œâ”€â”€ scenario_config_manager.py # Iterates and validates scenarios
    â”‚   â”œâ”€â”€ technology_manager.py      # Technology manager interface
    â”‚   â””â”€â”€ technologies/              # Tech-specific implementations of TechnologyManager
    â”œâ”€â”€ payload/                    # Message structures that the benchmark supports
    â”œâ”€â”€ technology_loader/          # Handles technology-specific plugin dynamic loading
    â”œâ”€â”€ Dockerfile.base             # Base image with C++ build dependencies
    â”œâ”€â”€ Dockerfile.publisher        # Publisher-specific image (extends base)
    â””â”€â”€ Dockerfile.consumer         # Consumer-specific image (extends base)
    technologies/                # Tech-specific Docker images + IPublisher/IConsumer implementations
    â”œâ”€â”€ arrowflight_bin_p2p/         # Arrow Flight (binary payload) implementation
    â”œâ”€â”€ arrowflight_p2p/             # Arrow Flight implementation
    â”œâ”€â”€ kafka_p2p/                   # Kafka implementation
    â”œâ”€â”€ nats_p2p/                    # NATS implementation
    â”œâ”€â”€ rabbitmq_p2p/                # RabbitMQ implementation
    â””â”€â”€ zeromq_p2p/                  # ZeroMQ implementation
    test_scenarios/              # Scenario configuration JSON files
    â”œâ”€â”€ quick_test.json             # Scenario config: topics, producers, rate, etc.
    â””â”€â”€ ...                         # Additional experimental configurations
    logs/                        # Benchmark log files organized per scenario and technology
    diagrams/                    # Architecture diagrams (PlantUML)
    third_party_libs/            # Vendored dependencies (e.g., spsc_queue, thread_pool)
    build/                       # CMake build artifacts
    app.py                       # CLI entry point for analysis helpers
    execute_experiments.py       # Entry point to execute an experiment (mode + duration_messages)
    benchmark_scenarios.json     # Parameterizes scenario configuration and technologies to run
    build_all_images.bat         # Builds Docker images for all technologies
    clean_log_tech.bat           # Removes technology-specific log folders
    CMakeLists.txt               # Root CMake configuration
    requirements.txt
    pyproject.toml
    README.md
    setup_instructions.md

---

## âš™ï¸ Technologies

Each messaging technology lives in its own subdirectory under `technologies/`. Each implementation must:
- Extend `IPublisher` and `IConsumer`
- Register itself via the factory
- Compile into a shared object

Each also gets its own Dockerfiles for consumer/publisher images.

Current implementations include Arrow Flight, Kafka, NATS, RabbitMQ, and ZeroMQ, with both standard and binary-payload variants where applicable.

---

## ğŸ“¦ Outputs & Logs

- Logs are written under `logs/` and grouped by scenario and technology.
- The analysis helpers in `analysis/` can load these logs and produce charts for latency, throughput, and resource metrics.

---

## ğŸ“Œ Notes

- Have the combinatory explosion threat in mind when designing scenario configuration dimension parameters.
- Technology-specific implementations belong to 2 scopes: their `technology` subfolder for messaging interfaces definition and registration, and in the `core/orchestrator/technology_loader/` folder for technology-specific global setup handling.
- The orchestrator uses Docker Python API â€” make sure there is access to the Docker daemon.
- Large scenario grids can grow quickly; tune sizes, rates, and durations to keep runs manageable.

---

## ğŸ§  For Extension

When adding a new tech or extending the scenario model:
- Follow the interface and registration pattern â€” nothing in `core/` should need to change except for the addition of a new python module in `core/orchestrator/technology_loader/`.
- Keep Docker images lean: start from `Dockerfile.base`, add only whatâ€™s necessary.
- Update the orchestrator only if scenario structure or orchestration behavior needs to evolve.

---

